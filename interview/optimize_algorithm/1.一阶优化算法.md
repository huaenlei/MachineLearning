# 一阶优化算法
## 1. 概述

最小化经验风险为代价函数在训练集上的平均:

$J(w)=\frac{1}{m}\sum_{i=1}^mL(f(x^i;w),y^i)$

## 2. 随机梯度下降(SGD)

Require: \alpha_k : 第k轮学习率 <br>
Require: 初始化参数w <br>
while 未达到条件 do <br>
&emsp;mini-batch $D = {(x^1,y^1),...,(x^m,y^m)}$ <br>
&emsp;计算梯度：$g=\frac{1}{m}\triangledown_wL(f(x^i;w),y^i)$<br>
&emsp;更新: $w = w - \alpha_kg$<br>
end while


## 3. 基于动量算法(Momentum)
重要的一点就是指数的加权平均,添加速度v

$v = \beta{v} + (1-\beta)g$ 指数加权<br>
$w = w - \alpha_k.v$

## 4. Nesterov动量
仅仅是在计算梯度直接更新w参数

Require: \alpha_k : 第k轮学习率,动量参数$\beta$ <br>
Require: 初始化参数w, 速度v <br>
while 未达到条件 do <br>
&emsp;mini-batch $D = {(x^1,y^1),...,(x^m,y^m)}$ <br>
&emsp;$w=w+\beta{v}$  //重要部分 <br>
&emsp;计算梯度：$g=\frac{1}{m}\triangledown_wL(f(x^i;w),y^i)$<br>
&emsp;$v = \beta{v} + (1-\beta)g$ 指数加权<br>
&emsp;更新: $w = w - \alpha_kv$<br>
end while

## 5. Adagrad
属于自适应算法，随着学习的轮数自动的更改学习率，Adagrad的算法是累计梯度的平方和从而更改学习率。

Require: 全局学习率$\alpha$,初始化参数w<br>
Require: $\delta$：非常小常数<br>
Require: r = 0 : 累计梯度平方和<br>
while 未达到条件 do<br>
&emsp;mini-batch $D = {(x^1,y^1),...,(x^m,y^m)}$ <br>
&emsp;计算梯度：$g=\frac{1}{m}\triangledown_wL(f(x^i;w),y^i)$<br>
&emsp;$r=r+g.g$<br>
&emsp;$\triangledown{w}=-\frac{\alpha}{\sqrt{r}+\delta}.g$<br>
&emsp;更新$w=w-\triangledown{w}$<br>
end while

Adagrad算法对凸优化算法有较好的解决，但实验证明对于深度网络而言，从开始累计梯度平方会导致学习率过低或过量减少，从而后期优化效果较差。为了解决这个问题，可以保留近期的梯度，而遗忘较远的梯度，因此就有了RMSProp

## 6. RMSProp
$r=pr_{i-1}+(1-p).g^2$对Adagrad的r进行替代<br>
$\triangledown{w}=\frac{\alpha}{\sqrt{\delta+r}}.g$

## 7. Adam
adam可以看做是对Momentum和RMSProp的合体，进行了一阶和二阶的整合

Require: 学习率 $\alpha$(默认0.001)<br>
Require: 矩估计衰减率 p_1(0.9), p_2(0.999)<br>
Require: 初始化参数w, 一阶矩和二阶矩变量s=0,r=0,轮数t=0
while 为满足条件 do<br>
&emsp;mini-batch $D = {(x^1,y^1),...,(x^m,y^m)}$ <br>
&emsp;计算梯度：$g=\frac{1}{m}\triangledown_wL(f(x^i;w),y^i)$<br>
&emsp;t=t+1<br>
&emsp;更新有偏一阶矩: $s=p_1.s+(1-p_1).g$<br>
&emsp;更新有偏二阶矩: $r=p_2.r+(1-p_2).g^2$<br>
&emsp;修正一阶矩: $s = \frac{s}{1-p_1^t}$<br>
&emsp;修正二阶矩: $r = \frac{r}{1-p_2^t}$<br>
&emsp;$\triangledown{w}=-\alpha\frac{s}{\sqrt{r}+
\delta}$<br>
&emsp;$w=w+\triangledown{w}$<br>
end while

    