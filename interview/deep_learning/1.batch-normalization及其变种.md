
# batch-normalization及其变种

神经网络中有各种归一化算法：Batch Normalization (BN)、Layer Normalization (LN)、Instance Normalization (IN)、Group Normalization (GN)。从公式看它们都差不多，如 (1) 所示：无非是减去均值，除以标准差，再施以线性映射。

$$y = r(\frac{x-u}{\sigma}) + \beta$$


## 1. batch normalization
一种是说BN能够解决“Internal Covariate Shift”这种问题。简单理解就是随着层数的增加，中间层的输出会发生“漂移”。另外一种说法是：BN能够解决梯度弥散。通过将输出进行适当的缩放，可以缓解梯度消失的状况。

### 1.1 什么是Internal Covariate Shift

网络中参数变化而引起**内部结点数据分布**发生变化的这一过程被称作Internal Covariate Shift

### 1.2 Internal Covariate Shift会带来什么问题？

sigmoid，tanh激活函数很容易陷入饱和区。

1. 上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低
2. 网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度

$$z = w * x$$
$$a = g(z)$$

随着网络的训练w增大，z增大，因此z很容易陷入饱和区，导致梯度接近于0.

对于激活函数梯度饱和问题，有两种解决思路。

- 第一种就是更为非饱和性激活函数，例如线性整流函数ReLU可以在一定程度上解决训练进入梯度饱和区的问题。
- 另一种思路是，我们可以让激活函数的输入分布保持在一个稳定状态来尽可能避免它们陷入梯度饱和区，这也就是Normalization的思路。

### 1.3 BN的优势

1. BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度: BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。
2. BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定
3. BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题
4. BN具有一定的正则化效果:由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。

## 2. layer normalization

NLP领域中，LN更为合适。

如果我们将一批文本组成一个batch，那么BN的操作方向是，对每句话的第一个词进行操作。但语言文本的复杂性是很高的，任何一个词都有可能放在初始位置，且词序可能并不影响我们对句子的理解。而BN是针对每个位置进行缩放，这不符合NLP的规律。

而LN则是针对一句话进行缩放的，且LN一般用在第三维度，如[batchsize, seq_len, dims]中的dims，一般为词向量的维度，或者是RNN的输出维度等等，这一维度各个特征的量纲应该相同。因此也不会遇到上面因为特征的量纲不同而导致的缩放问题。

## 3. batch layer normalization的不同点

batch是“竖”着来的，各个维度做归一化，所以与batch size有关系。
layer是“横”着来的，对一个样本，不同的神经元neuron间做归一化。

![batch-layer](https://pic1.zhimg.com/80/b710a578f73f478e3414d82e11c3055c_1440w.jpg)


## 参考资料
- [Batch Normalization原理与实战](https://zhuanlan.zhihu.com/p/34879333)
- [如何区分并记住常见的几种 Normalization 算法](https://zhuanlan.zhihu.com/p/69659844)