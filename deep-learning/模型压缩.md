
- [模型压缩方法综述](#%e6%a8%a1%e5%9e%8b%e5%8e%8b%e7%bc%a9%e6%96%b9%e6%b3%95%e7%bb%bc%e8%bf%b0)
  - [网络剪枝(network pruning)](#%e7%bd%91%e7%bb%9c%e5%89%aa%e6%9e%9dnetwork-pruning)
  - [知识蒸馏(knowledge distillation)](#%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8fknowledge-distillation)
  - [参数量化(parameter quantization)](#%e5%8f%82%e6%95%b0%e9%87%8f%e5%8c%96parameter-quantization)
  - [架构设计(architecture design)](#%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1architecture-design)
  - [动态计算(dynamic computation)](#%e5%8a%a8%e6%80%81%e8%ae%a1%e7%ae%97dynamic-computation)
  - [参考资料](#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99)

# 模型压缩方法综述

## 网络剪枝(network pruning)

1. 训练一个比较大的网络
2. 评估weight或者neuron的重要性
3. 删除重要性低的权重或者神经元
4. fine-turning
5. 符合预期结束，否则重复2

**为什么不开始就用小的网络去训练**
因为小的网络难以训练，而大的网络容易进行优化。


## 知识蒸馏(knowledge distillation)

思想是使用一个大的网络去训练一个模型，然后使用一个小的网络取学习大的网络的结果，比如分类问题，大的网络输出的是一个向量，每个类别都有概率[0.9,0.05, 0.05]，小的网络就学习这个向量，而不是学习真实的标签[1,0,0]。

## 参数量化(parameter quantization)

- 使用低bit代替高bit的(如float32替代float64)
- 二值网络

## 架构设计(architecture design)

- 全连接层 N*M(10, 1000)，可以添加一个更少参数的隐含层，如 N*V*M(10,5,1000)这种方式可以大量进行全连接成的参数
- cnn: 将标准的cnn转换成depth-wise cnn和point-wise cnn的两种操作，可以减少参数量

## 动态计算(dynamic computation)

- 训练多个分类器，在不同的时机使用
- 在比较深层的网络中间添加分类层


## 参考资料

- [深度学习模型压缩与加速综述
SIGAI](https://zhuanlan.zhihu.com/p/67871864)
- 机器学习(李宏毅)